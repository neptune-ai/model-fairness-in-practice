{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>jail_time</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>compas_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  juv_fel_count  juv_misd_count  juv_other_count  priors_count  \\\n",
       "0   69              0               0                0             0   \n",
       "1   34              0               0                0             0   \n",
       "2   24              0               0                1             4   \n",
       "3   23              0               1                0             1   \n",
       "4   43              0               0                0             2   \n",
       "\n",
       "   jail_time  sex  race  c_charge_degree  decile_score  two_year_recid  \\\n",
       "0        0.0    1     1                1             1               0   \n",
       "1       10.0    1     2                1             3               1   \n",
       "2        1.0    1     2                1             4               1   \n",
       "3        0.0    1     2                1             8               0   \n",
       "4        0.0    1     1                1             1               0   \n",
       "\n",
       "   compas_score  \n",
       "0           0.1  \n",
       "1           0.3  \n",
       "2           0.4  \n",
       "3           0.8  \n",
       "4           0.1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from utils import make_dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "TARGET_COLS = 'two_year_recid'\n",
    "COMPAS_SCORES_COLS = 'decile_score'\n",
    "NUMERICAL_FEATURE_COLS = ['age',\n",
    "                          'juv_fel_count','juv_misd_count','juv_other_count',\n",
    "                          'priors_count','jail_time']\n",
    "CATEGORICAL_FEATURE_COLS = ['sex','race',\n",
    "                            'c_charge_degree']\n",
    "FEATURE_NAMES = NUMERICAL_FEATURE_COLS+CATEGORICAL_FEATURE_COLS\n",
    "\n",
    "PROTECTED_COLS = ['sex','race']\n",
    "\n",
    "data = pd.read_csv('../data/processed/compas-scores-two-years-processed.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['compas_class'] = (data['decile_score']<6).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7012452309472434"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc = roc_auc_score(test[TARGET_COLS], test['compas_score'])\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright (c) 2019, Neptune Labs Sp. z o.o.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics.classification_metric import ClassificationMetric\n",
    "import neptune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from neptunecontrib.monitoring.utils import send_figure\n",
    "\n",
    "\n",
    "def log_fairness_classification_metrics(y_true, y_pred_class, sensitive,\n",
    "                                        favorable_label, unfavorable_label,\n",
    "                                        privileged_groups, unprivileged_groups,\n",
    "                                        experiment=None, prefix=''):\n",
    "    \"\"\"Creates fairness metric charts, calculates fairness classification metrics and logs them to Neptune.\n",
    "\n",
    "    Class-based metrics that are logged: 'true_positive_rate_difference','false_positive_rate_difference',\n",
    "    'false_omission_rate_difference', 'false_discovery_rate_difference', 'error_rate_difference',\n",
    "    'false_positive_rate_ratio', 'false_negative_rate_ratio', 'false_omission_rate_ratio',\n",
    "    'false_discovery_rate_ratio', 'error_rate_ratio', 'average_odds_difference', 'disparate_impact',\n",
    "    'statistical_parity_difference', 'equal_opportunity_difference', 'theil_index',\n",
    "    'between_group_theil_index', 'between_all_groups_theil_index', 'coefficient_of_variation',\n",
    "    'between_group_coefficient_of_variation', 'between_all_groups_coefficient_of_variation',\n",
    "    'generalized_entropy_index', 'between_group_generalized_entropy_index',\n",
    "    'between_all_groups_generalized_entropy_index'\n",
    "\n",
    "    Charts are logged to the 'metric_by_group' channel: 'confusion matrix', 'TPR', 'TNR', 'FPR', 'FNR', 'PPV', 'NPV',\n",
    "    'FDR', 'FOR', 'ACC', 'error_rate', 'selection_rate', 'power', 'precision', 'recall',\n",
    "    'sensitivity', 'specificity'.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like, shape (n_samples)): Ground truth (correct) target values.\n",
    "        y_pred_class (array-like, shape (n_samples)): Class predictions with values 0 or 1.\n",
    "        sensitive (pandas.DataFrame, shape (n_samples, k)): datafame containing only sensitive columns.\n",
    "        favorable_label (str or int): label that is favorable, brings positive value to a person being classified.\n",
    "        unfavorable_label (str or int): label that is unfavorable, brings positive value to a person being classified.\n",
    "        privileged_groups (dict): dictionary with column names and list of values for those columns that\n",
    "           belong to the privileged groups.\n",
    "        unprivileged_groups (dict): dictionary with column names and list of values for those columns that\n",
    "           belong to the unprivileged groups.\n",
    "        experiment(`neptune.experiments.Experiment`): Neptune experiment. Default is None.\n",
    "        prefix(str): Prefix that will be added before metric name when logged to Neptune.\n",
    "\n",
    "    Examples:\n",
    "        Train the model and make predictions on test.\n",
    "        Log metrics and performance curves to Neptune::\n",
    "\n",
    "            import neptune\n",
    "            from neptunecontrib.monitoring.fairness import log_fairness_classification_metrics\n",
    "\n",
    "            neptune.init()\n",
    "            with neptune.create_experiment():\n",
    "                log_fairness_classification_metrics(y_test, y_test_pred_class, test['race'],\n",
    "                                                    favorable_label='granted_parole',\n",
    "                                                    unfavorable_label='not_granted_parole',\n",
    "                                                    privileged_groups={'race':['Caucasian']},\n",
    "                                                    privileged_groups={'race':['African-American','Hispanic]},\n",
    "                                                    )\n",
    "\n",
    "        Check out this experiment https://ui.neptune.ml/jakub-czakon/model-fairness/e/MOD-92/logs.\n",
    "\n",
    "    \"\"\"\n",
    "    _exp = experiment if experiment else neptune\n",
    "\n",
    "    bias_info = {'favorable_label': favorable_label,\n",
    "                 'unfavorable_label': unfavorable_label,\n",
    "                 'protected_columns': sensitive.columns.tolist()}\n",
    "\n",
    "    privileged_info = _fmt_priveleged_info(privileged_groups, unprivileged_groups)\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    data['ground_truth'] = y_true.values\n",
    "    data['prediction'] = y_pred_class.values\n",
    "    for col in sensitive.columns:\n",
    "        data[col] = sensitive[col].values\n",
    "\n",
    "    ground_truth_test = _make_dataset(data, 'ground_truth', **bias_info, **privileged_info)\n",
    "    prediction_test = _make_dataset(data, 'prediction', **bias_info, **privileged_info)\n",
    "\n",
    "    clf_metric = ClassificationMetric(ground_truth_test, prediction_test, **privileged_info)\n",
    "\n",
    "    _log_fairness_metrics(clf_metric, _exp, prefix)\n",
    "\n",
    "    fig = _plot_confusion_matrix_by_group(clf_metric, figsize=(12, 4))\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    send_figure(fig, channel_name=prefix + 'metrics_by_group')\n",
    "\n",
    "    group_metrics = ['TPR', 'TNR', 'FPR', 'FNR', 'PPV', 'NPV', 'FDR', 'FOR',\n",
    "                     'ACC', 'error_rate', 'selection_rate', 'power',\n",
    "                     'precision', 'recall', 'sensitivity', 'specificity']\n",
    "\n",
    "    for metric_name in group_metrics:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        _plot_performance_by_group(clf_metric, metric_name, ax)\n",
    "        send_figure(fig, experiment=_exp, channel_name=prefix + 'metrics_by_group')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def _make_dataset(data, outcome, protected_columns,\n",
    "                  privileged_groups, unprivileged_groups,\n",
    "                  favorable_label, unfavorable_label):\n",
    "    df = data.copy()\n",
    "    df['outcome'] = data[outcome].values\n",
    "\n",
    "    dataset = BinaryLabelDataset(df=df, label_names=['outcome'], protected_attribute_names=protected_columns,\n",
    "                                 favorable_label=favorable_label, unfavorable_label=unfavorable_label,\n",
    "                                 unprivileged_protected_attributes=unprivileged_groups)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _fmt_priveleged_info(privileged_groups, unprivileged_groups):\n",
    "    privileged_info = {}\n",
    "    for name, group in zip(['privileged_groups', 'unprivileged_groups'],\n",
    "                           [privileged_groups, unprivileged_groups]):\n",
    "        privileged_info[name] = []\n",
    "        for k, values in group.items():\n",
    "            for v in values:\n",
    "                privileged_info[name].append({k: v})\n",
    "\n",
    "    return privileged_info\n",
    "\n",
    "\n",
    "def _log_fairness_metrics(aif_metric, experiment, prefix):\n",
    "    func_dict = {\n",
    "        'true_positive_rate_difference': aif_metric.true_positive_rate_difference,\n",
    "        'false_positive_rate_difference': aif_metric.false_positive_rate_difference,\n",
    "        'false_omission_rate_difference': aif_metric.false_omission_rate_difference,\n",
    "        'false_discovery_rate_difference': aif_metric.false_discovery_rate_difference,\n",
    "        'error_rate_difference': aif_metric.error_rate_difference,\n",
    "\n",
    "        'false_positive_rate_ratio': aif_metric.false_positive_rate_ratio,\n",
    "        'false_negative_rate_ratio': aif_metric.false_negative_rate_ratio,\n",
    "        'false_omission_rate_ratio': aif_metric.false_omission_rate_ratio,\n",
    "        'false_discovery_rate_ratio': aif_metric.false_discovery_rate_ratio,\n",
    "        'error_rate_ratio': aif_metric.error_rate_ratio,\n",
    "\n",
    "        'average_odds_difference': aif_metric.average_odds_difference,\n",
    "\n",
    "        'disparate_impact': aif_metric.disparate_impact,\n",
    "        'statistical_parity_difference': aif_metric.statistical_parity_difference,\n",
    "        'equal_opportunity_difference': aif_metric.equal_opportunity_difference,\n",
    "        'theil_index': aif_metric.theil_index,\n",
    "        'between_group_theil_index': aif_metric.between_group_theil_index,\n",
    "        'between_all_groups_theil_index': aif_metric.between_all_groups_theil_index,\n",
    "        'coefficient_of_variation': aif_metric.coefficient_of_variation,\n",
    "        'between_group_coefficient_of_variation': aif_metric.between_group_coefficient_of_variation,\n",
    "        'between_all_groups_coefficient_of_variation': aif_metric.between_all_groups_coefficient_of_variation,\n",
    "\n",
    "        'generalized_entropy_index': aif_metric.generalized_entropy_index,\n",
    "        'between_group_generalized_entropy_index': aif_metric.between_group_generalized_entropy_index,\n",
    "        'between_all_groups_generalized_entropy_index': aif_metric.between_all_groups_generalized_entropy_index}\n",
    "\n",
    "    for name, func in func_dict.items():\n",
    "        score = func()\n",
    "        experiment.log_metric(prefix + name, score)\n",
    "\n",
    "\n",
    "def _plot_confusion_matrix_by_group(aif_metric, figsize=None):\n",
    "    if not figsize:\n",
    "        figsize = (18, 4)\n",
    "\n",
    "    cmap = plt.get_cmap('Blues')\n",
    "    fig, axs = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "    axs[0].set_title('all')\n",
    "    cm = _format_aif360_to_sklearn(aif_metric.binary_confusion_matrix(privileged=None))\n",
    "    sns.heatmap(cm, cmap=cmap, annot=True, fmt='g', ax=axs[0])\n",
    "    axs[0].set_xlabel('predicted values')\n",
    "    axs[0].set_ylabel('actual values')\n",
    "\n",
    "    axs[1].set_title('privileged')\n",
    "    cm = _format_aif360_to_sklearn(aif_metric.binary_confusion_matrix(privileged=True))\n",
    "    sns.heatmap(cm, cmap=cmap, annot=True, fmt='g', ax=axs[1])\n",
    "    axs[1].set_xlabel('predicted values')\n",
    "    axs[1].set_ylabel('actual values')\n",
    "\n",
    "    axs[2].set_title('unprivileged')\n",
    "    cm = _format_aif360_to_sklearn(aif_metric.binary_confusion_matrix(privileged=False))\n",
    "    sns.heatmap(cm, cmap=cmap, annot=True, fmt='g', ax=axs[2])\n",
    "    axs[2].set_xlabel('predicted values')\n",
    "    axs[2].set_ylabel('actual values')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def _plot_performance_by_group(aif_metric, metric_name, ax=None):\n",
    "    performance_metrics = ['TPR', 'TNR', 'FPR', 'FNR', 'PPV', 'NPV', 'FDR', 'FOR', 'ACC']\n",
    "\n",
    "    func_dict = {'selection_rate': lambda x: aif_metric.selection_rate(privileged=x),\n",
    "                 'precision': lambda x: aif_metric.precision(privileged=x),\n",
    "                 'recall': lambda x: aif_metric.recall(privileged=x),\n",
    "                 'sensitivity': lambda x: aif_metric.sensitivity(privileged=x),\n",
    "                 'specificity': lambda x: aif_metric.specificity(privileged=x),\n",
    "                 'power': lambda x: aif_metric.power(privileged=x),\n",
    "                 'error_rate': lambda x: aif_metric.error_rate(privileged=x)}\n",
    "\n",
    "    if not ax:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    if metric_name in performance_metrics:\n",
    "        metric_func = lambda x: aif_metric.performance_measures(privileged=x)[metric_name]\n",
    "    elif metric_name in func_dict.keys():\n",
    "        metric_func = func_dict[metric_name]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['Group'] = ['all', 'priveleged', 'unpriveleged']\n",
    "    df[metric_name] = [metric_func(group) for group in [None, True, False]]\n",
    "\n",
    "    sns.barplot(x='Group', y=metric_name, data=df, ax=ax)\n",
    "    ax.set_title('{} by group'.format(metric_name))\n",
    "    ax.set_xlabel(None)\n",
    "\n",
    "    _add_annotations(ax)\n",
    "\n",
    "\n",
    "def _add_annotations(ax):\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(format(p.get_height(), '.3f'),\n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center',\n",
    "                    xytext=(0, -10), textcoords='offset points')\n",
    "\n",
    "\n",
    "def _format_aif360_to_sklearn(aif360_mat):\n",
    "    return np.array([[aif360_mat['TN'], aif360_mat['FP']],\n",
    "                     [aif360_mat['FN'], aif360_mat['TP']]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ml/jakub-czakon/model-fairness/e/MOD-97\n"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "from neptunecontrib.api.utils import get_filepaths\n",
    "\n",
    "neptune.init('jakub-czakon/model-fairness')\n",
    "neptune.create_experiment(name='compas',tags=['trash', 'race'],\n",
    "                          upload_source_files=get_filepaths(extensions=['.py','.ipynb']))\n",
    "\n",
    "neptune.log_metric('roc_auc',roc_auc)\n",
    "log_fairness_classification_metrics(test['two_year_recid'], test['compas_class'], test[['race']],\n",
    "                                    favorable_label=0, unfavorable_label=1,\n",
    "                                    privileged_groups = {'race':[3]},unprivileged_groups={'race':[1,2,4,5,6]}\n",
    "                                    )\n",
    "\n",
    "neptune.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_fairness",
   "language": "python",
   "name": "model_fairness"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
